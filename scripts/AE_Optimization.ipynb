{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_csv(folder_path, csv_file):\n",
    "    '''\n",
    "    Plots the validation loss for a specific CSV file.\n",
    "\n",
    "        Parameters:\n",
    "                folder_path (string): Path to the folder containing the CSV files\n",
    "                csv_file (string): Name of the CSV file\n",
    "    '''\n",
    "    # Read the CSV file\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract the epochs and validation loss as numpy arrays\n",
    "    epochs = data['Epoch'].values\n",
    "    validation_loss = data['Validation_Loss'].values\n",
    "    seed = data[\"SEED\"][0]\n",
    "    learning_rate = data[\"AE_LEARNING_RATE\"][0]\n",
    "    batch_size = data[\"AE_BATCH_SIZE\"][0]\n",
    "    test_loss = data[\"Test_Loss\"][0]\n",
    "    \n",
    "    # Plot the graph\n",
    "    plt.plot(epochs, validation_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title(f'Validation Loss - {seed} - {learning_rate} - {batch_size}')\n",
    "    plt.suptitle(f'Test Loss: {test_loss}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specific_combination(folder_path, specific_lr, specific_bs):\n",
    "    '''\n",
    "    Plots the validation loss per seed & average for a combination of hyperparameters\n",
    "    \n",
    "        Parameters:\n",
    "                folder_path (string): Path to the folder containing the CSV files\n",
    "                specific_lr (float): learning rate\n",
    "                specific_bs (int): batch size\n",
    "    '''\n",
    "\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Filter out the CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    list_with_best_combinations = []\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = pd.read_csv(file_path)\n",
    "        AE_learning_rate = data[\"AE_LEARNING_RATE\"][0]\n",
    "        AE_batch_size = data[\"AE_BATCH_SIZE\"][0]\n",
    "        AE_validation_loss = data[\"Validation_Loss\"].values\n",
    "        AE_seed = data[\"SEED\"][0]\n",
    "        AE_dataset_name = data[\"Dataset\"][0]\n",
    "\n",
    "        if AE_learning_rate == specific_lr and AE_batch_size == specific_bs:\n",
    "            list_with_best_combinations.append((AE_seed, AE_validation_loss))\n",
    "\n",
    "\n",
    "    list_with_best_combinations = sorted(list_with_best_combinations, key=lambda x:x[0])\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Iterate through the seed-test_loss pairs\n",
    "    for seed, test_loss in list_with_best_combinations:\n",
    "        # Plot the test loss values for the current seed\n",
    "        ax.plot(test_loss, label=f\"Seed {seed}\")\n",
    "    \n",
    "    # Average over all seeds\n",
    "    # Extract the array from the tuple\n",
    "    array_data = [arr for _, arr in list_with_best_combinations]\n",
    "    # Calculate the average for each corresponding element in the arrays\n",
    "    averages = np.mean(array_data, axis=0)\n",
    "    ax.plot(averages, label=f\"Average\")\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Validation Loss')\n",
    "    ax.set_title(f'{AE_dataset_name}: Validation Loss - {specific_lr} - {specific_bs}')\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_combination(folder_path, round_numbers):\n",
    "    '''\n",
    "    Returns the best combination of hyperparameters\n",
    "\n",
    "        Parameters:\n",
    "                folder_path (string): Path to the folder containing the CSV files\n",
    "        Returns:\n",
    "                best_learning_rate (float): Best learning rate\n",
    "                best_batch_size (int): Best batch size\n",
    "    '''\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Filter out the CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    # Define the combinations of learning_rate and batch_size\n",
    "    learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.0005]\n",
    "    batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "    # Store results as tuples in list\n",
    "    results = []\n",
    "\n",
    "    # Iterate through the combinations (group by learning_rate and batch_size)\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            # initialize an empty list to store test losses for each combination\n",
    "            test_losses = []\n",
    "            seeds = []\n",
    "\n",
    "            # Iterate through the CSV files\n",
    "            for file in csv_files:\n",
    "                # Read the CSV file\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                AE_learning_rate = data[\"AE_LEARNING_RATE\"][0]\n",
    "                AE_batch_size = data[\"AE_BATCH_SIZE\"][0]\n",
    "                AE_test_loss = data[\"Test_Loss\"][0]\n",
    "                AE_dataset_name = data[\"Dataset\"][0]\n",
    "                AE_seed = data[\"SEED\"][0]\n",
    "\n",
    "                if AE_learning_rate == lr and AE_batch_size == bs:\n",
    "                    test_losses.append(AE_test_loss)\n",
    "                    seeds.append(AE_seed)\n",
    "                    # plot_single_csv(folder_path, file)\n",
    "            \n",
    "            # Calculate the mean test loss\n",
    "            if(len(test_losses) != 0):\n",
    "                assert len(set(seeds)) == 5, \"Number of unique seeds is not equal to 5 -> ERROR.\"\n",
    "                assert len(test_losses) == 5, \"Number of test losses is not equal to 5 -> ERROR.\"\n",
    "                mean_test_loss = np.mean(test_losses)\n",
    "                se_test_loss = stats.sem(test_losses)\n",
    "\n",
    "                # Calculate the 95% confidence interval for test accuracy\n",
    "                confidence_interval_loss = stats.t.interval(0.95, len(test_losses) - 1, loc=mean_test_loss, scale=se_test_loss)\n",
    "                conf_interval_half_loss = abs(confidence_interval_loss[1] - confidence_interval_loss[0]) / 2\n",
    "\n",
    "                assert mean_test_loss == (sum(test_losses)/len(test_losses)), \"Mean test loss is not equal to sum of test losses divided by number of test losses -> ERROR.\"\n",
    "                print(f\"Learning Rate: {lr}, Batch Size: {bs} || Mean Test Loss: {mean_test_loss}, SE Test Loss: {se_test_loss}, 95% Confidence Interval: {confidence_interval_loss}, 95% Confidence Interval Half: {conf_interval_half_loss}\")\n",
    "                results.append((lr, bs, mean_test_loss, conf_interval_half_loss, confidence_interval_loss))\n",
    "\n",
    "    # sort results by mean test loss\n",
    "    sorted_results = sorted(results, key=lambda x:x[2])\n",
    "    print(f\"len(sorted_results) = {len(sorted_results)}\")\n",
    "    print(f\"len(csv_files) = {len(csv_files)}\")\n",
    "    print(\"Sorted List:\")\n",
    "    print(sorted_results)\n",
    "    print(f\"Best Combination for {AE_dataset_name}:\")\n",
    "    best_learning_rate = sorted_results[0][0]\n",
    "    best_batch_size = sorted_results[0][1]\n",
    "    best_mean_test_loss = sorted_results[0][2]\n",
    "    best_conf_interval_half_loss = sorted_results[0][3]\n",
    "\n",
    "    if round_numbers:\n",
    "        # round to 4 decimal places\n",
    "        best_mean_test_loss = round(best_mean_test_loss, 4)\n",
    "        best_conf_interval_half_loss = round(best_conf_interval_half_loss, 4)\n",
    "    print(f\"Learning Rate: {best_learning_rate}, Batch Size: {best_batch_size}, Mean Test Loss: {best_mean_test_loss}, 95% Confidence Interval Half: {best_conf_interval_half_loss}\")\n",
    "    print(100*\"*\")\n",
    "\n",
    "    return best_learning_rate, best_batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "import itertools\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Computer Modern\"]})\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set(font=\"Computer Modern\")\n",
    "\n",
    "\n",
    "# Define the color palette for the models\n",
    "model_color_palette = sns.color_palette(\"colorblind\")\n",
    "relevant_model_names = [\"VQC_angle_list\", \"VQC_amplitude_list\", \"dressed_quantum_list\", \"sequent_quantum_list\", \"NN_with_compressed_input_list\", \"NN_with_original_input_list\", \"DQC_classical\", \"Sequent_classical\"]\n",
    "\n",
    "# Create a custom color dictionary to map each model name to its corresponding color\n",
    "model_colors = dict(zip(relevant_model_names, model_color_palette))\n",
    "print(model_colors)\n",
    "\n",
    "sns.set(style=\"whitegrid\", font=\"serif\", font_scale=2)\n",
    "\n",
    "\n",
    "\n",
    "def plot_specific_combination_2(folder_path, specific_lr, specific_bs):\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Filter out the CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    data_list = []\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = pd.read_csv(file_path)\n",
    "        AE_learning_rate = data[\"AE_LEARNING_RATE\"][0]\n",
    "        AE_batch_size = data[\"AE_BATCH_SIZE\"][0]\n",
    "        AE_validation_loss = data[\"Validation_Loss\"].values\n",
    "        AE_seed = data[\"SEED\"][0]\n",
    "        AE_dataset_name = data[\"Dataset\"][0]\n",
    "\n",
    "        if AE_learning_rate == specific_lr and AE_batch_size == specific_bs:\n",
    "            # Add multiple rows for each epoch and its corresponding validation loss\n",
    "            for epoch, loss in enumerate(AE_validation_loss, start=1):\n",
    "                data_list.append((AE_seed, epoch, loss))\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=[\"seed\", \"epoch\", \"validation loss\"])\n",
    "\n",
    "    # Calculate the average validation loss and standard deviation for each epoch across all seeds\n",
    "    avg_and_std_per_epoch = df.groupby('epoch')['validation loss'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Create a line plot for the average validation loss with error band (standard deviation)\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    sns.lineplot(data=avg_and_std_per_epoch, x='epoch', y='mean', linewidth=2)\n",
    "    plt.fill_between(avg_and_std_per_epoch['epoch'], avg_and_std_per_epoch['mean'] - avg_and_std_per_epoch['std'],\n",
    "                     avg_and_std_per_epoch['mean'] + avg_and_std_per_epoch['std'], alpha=0.3)\n",
    "\n",
    "    '''\n",
    "    plt.xlim(left=0)#, right=max_epoch)\n",
    "    plt.xlim(right=500)\n",
    "    plt.ylim(bottom=0, top=0.06)\n",
    "    '''\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_specific_combination_smooth(folder_path, specific_lr, specific_bs):\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Filter out the CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    data_list = []\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = pd.read_csv(file_path)\n",
    "        AE_learning_rate = data[\"AE_LEARNING_RATE\"][0]\n",
    "        AE_batch_size = data[\"AE_BATCH_SIZE\"][0]\n",
    "        AE_validation_loss = data[\"Validation_Loss\"].values\n",
    "        AE_seed = data[\"SEED\"][0]\n",
    "        AE_dataset_name = data[\"Dataset\"][0]\n",
    "\n",
    "        if AE_learning_rate == specific_lr and AE_batch_size == specific_bs:\n",
    "            # Add multiple rows for each epoch and its corresponding validation loss\n",
    "            for epoch, loss in enumerate(AE_validation_loss, start=1):\n",
    "                data_list.append((AE_seed, epoch, loss))\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=[\"seed\", \"epoch\", \"validation loss\"])\n",
    "\n",
    "    avg_and_std_per_epoch = df.groupby('epoch')['validation loss'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Calculate smoothed validation loss using exponential moving average (EMA)\n",
    "    alpha = 0.6\n",
    "    avg_and_std_per_epoch['smoothed_mean'] = avg_and_std_per_epoch['mean'].ewm(alpha=alpha).mean()\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "\n",
    "    # Plot the smoothed average validation loss with error band (standard deviation)\n",
    "    sns.lineplot(x='epoch', y='smoothed_mean', data=avg_and_std_per_epoch, linewidth=2)\n",
    "    plt.fill_between(avg_and_std_per_epoch['epoch'],\n",
    "                     avg_and_std_per_epoch['smoothed_mean'] - avg_and_std_per_epoch['std'],\n",
    "                     avg_and_std_per_epoch['smoothed_mean'] + avg_and_std_per_epoch['std'], alpha=0.3)\n",
    "\n",
    "    plt.xlim(left=1)  # Start the x-axis from epoch 1\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Validation Loss')\n",
    "    plt.title('Smoothed Average Validation Loss with Standard Deviation over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Computer Modern\"]})\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "# Define the color palette for the models\n",
    "model_color_palette = sns.color_palette(\"colorblind\")\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\", font=\"Computer Modern\", font_scale=2)\n",
    "\n",
    "\n",
    "def plot_specific_combination_smooth_mean_std(folder_path, specific_lr, specific_bs):\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Filter out the CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    data_list = []\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = pd.read_csv(file_path)\n",
    "        AE_learning_rate = data[\"AE_LEARNING_RATE\"][0]\n",
    "        AE_batch_size = data[\"AE_BATCH_SIZE\"][0]\n",
    "        AE_validation_loss = data[\"Validation_Loss\"].values\n",
    "        AE_seed = data[\"SEED\"][0]\n",
    "        AE_dataset_name = data[\"Dataset\"][0]\n",
    "\n",
    "        if AE_learning_rate == specific_lr and AE_batch_size == specific_bs:\n",
    "            # Add multiple rows for each epoch and its corresponding validation loss\n",
    "            for epoch, loss in enumerate(AE_validation_loss, start=1):\n",
    "                data_list.append((AE_seed, epoch, loss))\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=[\"seed\", \"epoch\", \"validation loss\"])\n",
    "    avg_and_std_per_epoch = df.groupby('epoch')['validation loss'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    alpha = 0.6\n",
    "    # Calculate smoothed validation loss and standard deviation using exponential moving average (EMA)\n",
    "    avg_and_std_per_epoch['smoothed_mean'] = avg_and_std_per_epoch['mean'].ewm(alpha=alpha).mean()\n",
    "    avg_and_std_per_epoch['smoothed_std'] = avg_and_std_per_epoch['std'].ewm(alpha=alpha).mean()\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "\n",
    "    # Plot the smoothed average validation loss with error band (standard deviation)\n",
    "    sns.lineplot(x='epoch', y='smoothed_mean', data=avg_and_std_per_epoch, linewidth=1.5, palette=model_color_palette)\n",
    "    plt.fill_between(avg_and_std_per_epoch['epoch'],\n",
    "                     avg_and_std_per_epoch['smoothed_mean'] - avg_and_std_per_epoch['smoothed_std'],\n",
    "                     avg_and_std_per_epoch['smoothed_mean'] + avg_and_std_per_epoch['smoothed_std'], alpha=0.3)\n",
    "\n",
    "    plt.xlim(left = 0, right = 500)  \n",
    "    # plt.ylim(bottom = 0, top = 0.06)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Reconstruction Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_BanknoteAuthentication= os.path.join(os.getcwd(), \"AE_Optimization\", \"Banknote\")\n",
    "folder_path_BreastCancer = os.path.join(os.getcwd(), \"AE_Optimization\", \"Breast_Cancer\")\n",
    "folder_path_MNIST = os.path.join(os.getcwd(), \"AE_Optimization\", \"MNIST\")\n",
    "folder_path_Audio = os.path.join(os.getcwd(), \"AE_Optimization\", \"Audio_MNIST\")\n",
    "\n",
    "\n",
    "best_learning_rate_BanknoteAuthentication, best_batch_size_BanknoteAuthentication = best_combination(folder_path_BanknoteAuthentication, True)\n",
    "best_learning_rate_BreastCancer, best_batch_size_BreastCancer = best_combination(folder_path_BreastCancer, True)\n",
    "best_learning_rate_MNIST, best_batch_size_MNIST = best_combination(folder_path_MNIST, True)\n",
    "best_learning_rate_Audio, best_batch_size_Audio = best_combination(folder_path_Audio, True)\n",
    "\n",
    "print(\"Plot for best combination\")\n",
    "plot_specific_combination_smooth_mean_std(folder_path_BanknoteAuthentication, best_learning_rate_BanknoteAuthentication, best_batch_size_BanknoteAuthentication)\n",
    "plot_specific_combination_smooth_mean_std(folder_path_BreastCancer, best_learning_rate_BreastCancer, best_batch_size_BreastCancer)\n",
    "plot_specific_combination_smooth_mean_std(folder_path_MNIST, best_learning_rate_MNIST, best_batch_size_MNIST)\n",
    "plot_specific_combination_smooth_mean_std(folder_path_Audio, best_learning_rate_Audio, best_batch_size_Audio)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"Plot for all combinations\")\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.0005]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        plot_specific_combination(folder_path_Audio, lr, bs)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
